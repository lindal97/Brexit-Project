{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import geopandas as gpd\n",
    "\n",
    "from wordcloud import *\n",
    "from PIL import Image\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from gensim import *\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"D:/grad/MA thesis/data/meta stat\")\n",
    "sns.set(style = 'whitegrid')\n",
    "\n",
    "#basic setting: colour\n",
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"4\">Figure 1(a) - Figure 2: Metadata</fontsize> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1(a): user creation date. Note this is a version suitable for Latex output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#loading data\n",
    "users = pd.read_csv(\"user_created_at.csv\")\n",
    "user_slice = users.loc[3442:] #may be edited for other time slices\n",
    "user_slice[\"formatted\"] = [datetime.strptime(d, \"%Y/%m/%d\") for d in user_slice[\"DATE(user_created_at_formatted)\"]]\n",
    "rolling = user_slice[\"COUNT(*)\"].rolling(window=30)\n",
    "rolling_mean = rolling.mean()\n",
    "user_slice[\"monthly\"] = rolling_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.plot(user_slice['formatted'], user_slice['COUNT(*)'], label = \"Daily sum\", color = tableau20[0])\n",
    "ax1.plot(user_slice[\"formatted\"], user_slice[\"monthly\"],label = \"Monthly rolling average\",  \n",
    "         color = tableau20[2])\n",
    "ax1.legend(prop={'size': 15})\n",
    "ax1.grid(linestyle = 'dashed')\n",
    "ax1.spines[\"top\"].set_visible(False)       \n",
    "ax1.spines[\"right\"].set_visible(False)  \n",
    "\n",
    "plt.grid(linestyle = 'dashed')\n",
    "ax1.set_ylabel('Newly registered users',fontsize = 20)\n",
    "#setting time axis\n",
    "yearsFmt = mdates.DateFormatter('%Y-%m')\n",
    "ax1.xaxis.set_major_formatter(yearsFmt)\n",
    "ax1.set_xlabel('Date',fontsize = 20)\n",
    "#providing directions for some key incidents\n",
    "times = [\"2016-06-23\",\"2017-01-17\",\"2018-05-27\"]\n",
    "for t in times:\n",
    "    ax1.axvline(x = t, linewidth = 1, linestyle = \"dashed\")\n",
    "\n",
    "ax1.axvspan('2019-3-1', '2019-5-4', alpha=0.1, color='grey')\n",
    "ax1.axvspan('2018-11-13', '2018-12-10', alpha=0.1, color='grey')\n",
    "ax1.text(x = \"2016-06-18\", y = 320, horizontalalignment = 'right', s = \"Brexit\\nreferendum\", fontsize = 15)\n",
    "ax1.text(x = \"2017-01-15\", y = 400, horizontalalignment = 'right', s = \"Theresa May's\\nBrexit speech\",\n",
    "        fontsize = 15)\n",
    "ax1.text(x = \"2018-05-20\", y = 400, horizontalalignment = 'right',s = \"Parliment passed\\nwithdrawl act\",\n",
    "       fontsize = 15)\n",
    "ax1.text(x = \"2018-11-30\", y = 500, horizontalalignment = 'right',s = \"Period before the\\n(postponed) deal vote\",\n",
    "        fontsize = 15)\n",
    "ax1.text(x = \"2019-03-08\", y = 700, horizontalalignment = 'right', s = \"EU parliament election\\npropoganda\",\n",
    "               fontsize = 15)\n",
    "\n",
    "#plt.savefig(\"meta_1.png\")\n",
    "#plt.savefig(\"meta_1.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta data figure 1(b): daily number of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "tweets = pd.read_csv(\"number of tweets.csv\")\n",
    "tweet_rolling = tweets[\"Intrapolated\"].rolling(window=7)\n",
    "tweet_rolling_mean = tweet_rolling.mean()\n",
    "tweets[\"weekly\"] = tweet_rolling_mean\n",
    "tweets[\"formatted\"] = [datetime.strptime(d, \"%d/%m/%Y\") for d in tweets[\"DATE(created_at_formatted)\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "fig = plt.figure(figsize = (20,8))\n",
    "ax2 = fig.add_subplot(111)\n",
    "\n",
    "dates = list(tweets[\"formatted\"])\n",
    "tweet_num = list(tweets[\"Intrapolated\"])\n",
    "\n",
    "\n",
    "ax2.bar(tweets[\"formatted\"].values, tweets[\"Intrapolated\"].values, label = \"Daily sum\")\n",
    "ax2.plot(tweets[\"formatted\"].values, tweet_rolling_mean ,label = \"Weekly rolling average\", color = tableau20[2])\n",
    "ax2.legend(prop={'size': 15})\n",
    "ax2.grid(linestyle = 'dashed')\n",
    "ax2.spines[\"top\"].set_visible(False)       \n",
    "ax2.spines[\"right\"].set_visible(False)  \n",
    "\n",
    "ax2.set_xlabel('Date', fontsize = 20)\n",
    "ax2.set_ylabel('Number of tweets scraped', fontsize = 20)\n",
    "#plt.savefig(\"meta_2.png\")\n",
    "#plt.savefig(\"meta_2.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1(c): Bigram, with the tags manually tweaked a little bit...(selected the top 25 and deleted a meaningless one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "bigram_sample = pd.read_csv(\"sample_new.csv\", encoding = 'latin-1')\n",
    "\n",
    "with open(\"D:/grad/MA thesis/data/stopwords/smart-common-words.txt\", 'r') as f:\n",
    "    stopwords = f.read().split(\",\")\n",
    "\n",
    "def preprocess(text,stopwords):\n",
    "    result=[]\n",
    "    text = text.lower()\n",
    "    for token in utils.simple_preprocess(text) :\n",
    "        if token not in stopwords and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "def bigram_finder(data,stopwords):\n",
    "    tag_counter = {}\n",
    "    for i in data:\n",
    "        tag = preprocess(i, stopwords)\n",
    "        for t in tag:\n",
    "            try:\n",
    "                t = t.lower()\n",
    "                if t in tag_counter:\n",
    "                    tag_counter[t]  += 1\n",
    "                else:\n",
    "                    tag_counter[t] = 1\n",
    "            except:\n",
    "                pass\n",
    "    return tag_counter\n",
    "\n",
    "leave_tags = bigram_finder(bigram_sample[bigram_sample[\"leave_remain\"]>0][\"full_text\"], stopwords)\n",
    "remain_tags = bigram_finder(bigram_sample[bigram_sample[\"leave_remain\"]<0][\"full_text\"], stopwords)\n",
    "\n",
    "leave_top = {k: v for k, v in leave_tags.items() if v > 380}\n",
    "remain_top = {k: v for k, v in remain_tags.items() if v > 430}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax3.barh(range(len(leave_top)), [i*20 for i in leave_top.values()], align='center', color = tableau20[0])  \n",
    "ax3.barh(range(len(remain_top)), [-i*20 for i in remain_top.values()], align='center', color = tableau20[6])\n",
    "ax3.set_xlabel('Phrase frequency', fontsize = 20)\n",
    "\n",
    "rects = ax3.patches\n",
    "leave_labels = list(leave_top.keys())\n",
    "remain_labels = list(remain_top.keys())\n",
    "# For each bar: Place a label\n",
    "\n",
    "i = 0\n",
    "for rect in rects:\n",
    "    x_value = rect.get_width()\n",
    "    y_value = rect.get_y() + rect.get_height() / 2\n",
    "    space = 5\n",
    "    ha = 'left'\n",
    "    if x_value > 0:\n",
    "        label = leave_labels[i]\n",
    "    else:\n",
    "        space *= -1\n",
    "        ha = 'right'\n",
    "        label = remain_labels[i - 24]\n",
    "    ax3.annotate(\n",
    "        label,                      # Use `label` as label\n",
    "        (x_value, y_value),         # Place label at end of the bar\n",
    "        xytext=(space, 0),          # Horizontally shift label by `space`\n",
    "        textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "        va='center',                # Vertically center label\n",
    "        ha=ha,\n",
    "        fontsize = 15)\n",
    "    i += 1                      # Horizontally align label differently for\n",
    "\n",
    "ax3.get_yaxis().set_visible(False)\n",
    "ax3.grid(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "ax3.spines['left'].set_visible(False)\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ticks =  ax3.get_xticks()\n",
    "ax3.set_xticklabels([int(abs(tick)) for tick in ticks])\n",
    "ax3.legend(prop={'size': 15})\n",
    "#plt.savefig(\"meta_3.png\")\n",
    "#plt.savefig(\"meta_3.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2: Choropleth of all users with reported location in UK, breakdown by adminstrative constitutencies. \n",
    "Imported processed .shp file directly. Please see geocoding.py for the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_shp = gpd.read_file(\"./geocoding/GBP_new.shp\")\n",
    "uk_shp[\"log_num\"] = [math.log10(i) if i != 0 else 0 for i in uk_shp[\"real_point\"] ]\n",
    "uk_shp_nolondon = uk_shp.drop(index = 35)\n",
    "london = uk_shp.iloc[35,:]\n",
    "\n",
    "uk_shp.crs = {'init' :'epsg:4326'}\n",
    "uk_shp = uk_shp.to_crs({'init': 'epsg:3395'})\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.set_aspect('equal')\n",
    "ax4.set_ylim([6340000, 8550000])\n",
    "ax4.set_xlim([-1200000, 282411])\n",
    "uk_shp.plot(ax = ax4, color = 'white', edgecolor = \"black\")\n",
    "divider = make_axes_locatable(ax4)\n",
    "cax = divider.append_axes(\"left\", size=\"8%\", pad=0.1)\n",
    "cax.tick_params(labelsize=20)\n",
    "shp = uk_shp.plot(column = 'log_num',ax = ax4,  cmap = 'Blues',legend = True,cax = cax)\n",
    "ax4.axis(\"off\")\n",
    "\n",
    "#plt.savefig(\"meta_4.pdf\")\n",
    "#plt.savefig(\"meta_4.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">Figure 3: Wordclouds</font size> \n",
    "- Figure 3(a): Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pattern_finder(data, regexp, tag_counter):\n",
    "    for i in data:\n",
    "        tag = re.findall(regexp, i)\n",
    "        if tag is None:\n",
    "            continue\n",
    "        else:\n",
    "            for t in tag:\n",
    "                t = t.lower()\n",
    "                t = t[1:]\n",
    "                if t in tag_counter:\n",
    "                    tag_counter[t]  += 1\n",
    "                else:\n",
    "                    tag_counter[t] = 1\n",
    "    return tag_counter\n",
    "'''\n",
    "This part is from the example from wordcloud package document:\n",
    "https://amueller.github.io/word_cloud/auto_examples/\n",
    "    colored_by_group.html#sphx-glr-auto-examples-colored-by-group-py\n",
    "'''\n",
    "class SimpleGroupedColorFunc(object):\n",
    "    def __init__(self, color_to_words, default_color):\n",
    "        self.word_to_color = {word: color\n",
    "                              for (color, words) in color_to_words.items()\n",
    "                              for word in words}\n",
    "\n",
    "        self.default_color = default_color\n",
    "\n",
    "    def __call__(self, word, **kwargs):\n",
    "        return self.word_to_color.get(word, self.default_color)\n",
    "\n",
    "\n",
    "with open('sample.csv', newline='', encoding = 'utf-8') as csvfile:\n",
    "     sample = csv.reader(csvfile)\n",
    "     tag_counter = {}\n",
    "     corpus = \"\"\n",
    "     for row in sample:\n",
    "         tag_counter = pattern_finder(row, r'([#][\\w_-]+)',tag_counter)\n",
    "         \n",
    "\n",
    "''' the hashtag cloud'''\n",
    "#tag tendency is a preprocessed file with tag freqency & tag tendency(leave/remain). Please see other codes for the processing process.\n",
    "tag_tendency = pd.read_csv(\"tag_label.csv\", encoding = 'latin-1')\n",
    "colour = {\"red\":[], \"blue\":[]}\n",
    "\n",
    "for i in range(len(tag_tendency)):\n",
    "    if tag_tendency[\"tendency\"].iloc[i] == 1: \n",
    "        colour[\"red\"].append(tag_tendency[\"tag\"].iloc[i])\n",
    "    elif tag_tendency[\"tendency\"].iloc[i] == 3:\n",
    "        colour[\"blue\"].append(tag_tendency[\"tag\"].iloc[i])\n",
    "    else:\n",
    "        pass\n",
    "default_colour = 'grey'\n",
    "\n",
    "mask = np.array(Image.open('uk1.png'))                 \n",
    "wc = WordCloud(background_color=\"white\", max_words = 400, mask = mask, \n",
    "                   width=1600, height = 800)\n",
    "wc.generate_from_frequencies(tag_counter)\n",
    "grouped_color_func = SimpleGroupedColorFunc(colour, default_colour)\n",
    "wc.recolor(color_func=grouped_color_func)\n",
    "plt.imshow(wc, interpolation=\"spline16\")\n",
    "plt.axis(\"off\")\n",
    "#wc.to_file(\"hashtag_cloud.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3(b): Leave and remain tweet keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt', 'r') as f:\n",
    "    stopword = f.read()\n",
    "stopwords = stopword.split(\",\")\n",
    "\n",
    "def wc_generator(text_path, color_func, mask, savename, stopwords = stopwords, max_words = 400):\n",
    "    wc = WordCloud(background_color=\"white\", max_words = max_words, mask = mask, \n",
    "                   width=1600, height = 800, stopwords = stopwords\n",
    "                   )\n",
    "    text =  open(text_path, encoding = 'utf-8').read()\n",
    "    wc.generate(text)\n",
    "    plt.imshow(wc.recolor(color_func=color_func, random_state=3), interpolation=\"spline16\")\n",
    "    plt.axis(\"off\")\n",
    "    wc.to_file(savename)\n",
    "    return\n",
    "\n",
    "def red_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(10, 100%%, %d%%)\" % random.randint(40, 100)\n",
    "\n",
    "def blue_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(242, 100%%, %d%%)\" % random.randint(40, 100)\n",
    "\n",
    "#remain \n",
    "mask = np.array(Image.open('half_circle_left.jpg'))\n",
    "mask_right = np.array(Image.open('half_circle_right.jpg'))\n",
    "wc_generator('remain_text.txt', red_color_func, mask, 'remain_cloud.png')\n",
    "#leave\n",
    "wc_generator('leave_text.txt', blue_color_func, mask_right, 'leave_cloud.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">Figure 4(a) - 4(d): Sentiment</font size> \n",
    "- 4(a): Daily leave/remain tendency\n",
    "- 4(b): Daily avg sentiment of leave/remain tweets\n",
    "- 4(c): Sentiment of remain media + tweets\n",
    "- 4(d): Sentiment of leave media + tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed data\n",
    "twitter = pd.read_csv(\"leave_sentiment_count.csv\")\n",
    "newspaper = pd.read_csv(\"dailymail_senti.csv\")\n",
    "\n",
    "#Count avg daily tendency \n",
    "twitter[\"formatted\"] = [datetime.strptime(i, \"%Y/%m/%d\") for i in twitter[\"date\"]]\n",
    "newspaper[\"formatted\"] = [datetime.strptime(i, \"%Y/%m/%d\") for i in newspaper[\"date\"]]\n",
    "twitter = twitter.interpolate()\n",
    "newspaper = newspaper.interpolate()\n",
    "twitter = twitter.drop(index = [i for i in range(208,213)])\n",
    "rolling = newspaper[\"guard_senti\"].rolling(window=3)\n",
    "rolling_mean = rolling.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,7))\n",
    "#fig 4(a) with average leave/remain tendency\n",
    "axs1 = fig.add_subplot(111)\n",
    "axs1.plot(twitter[\"formatted\"], twitter[\"avg_leave_score\"].values, label = \"Daily pro-leave score\")\n",
    "axs1.legend(prop={'size': 15})\n",
    "axs1.set_ylabel(\"Pro-leave tendency\", fontsize = 20)\n",
    "axs1.set_xlabel(\"Date\", fontsize = 20)\n",
    "axs1.axhline(linewidth = 2, color = \"black\")\n",
    "axs1.grid(False)\n",
    "axs1.set_xlim(['2018-11-13', '2019-06-04'])\n",
    "axs1.spines[\"top\"].set_visible(False)       \n",
    "axs1.spines[\"right\"].set_visible(False)  \n",
    "plt.savefig(\"sentiment_1.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig 4(b): sentiment scores\n",
    "fig = plt.figure(figsize = (20,7))\n",
    "axs2 = fig.add_subplot(111)\n",
    "axs2.plot(twitter[\"formatted\"], twitter[\"remain_senti\"].values, label = \"Sentiment of pro-remain tweets\")\n",
    "axs2.plot(twitter[\"formatted\"], twitter[\"leave_senti\"].values, label = \"Sentiment of pro-leave tweets\")\n",
    "axs2.set_ylabel(\"Sentiment level\", fontsize = 20)\n",
    "axs2.set_xlabel(\"Date\", fontsize = 20)\n",
    "axs2.legend(prop={'size': 15})\n",
    "axs2.grid(False)\n",
    "axs2.set_xlim(['2018-11-13', '2019-06-04'])\n",
    "axs2.spines[\"top\"].set_visible(False)       \n",
    "axs2.spines[\"right\"].set_visible(False)  \n",
    "plt.savefig(\"sentiment_2.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig 4(c)\n",
    "fig = plt.figure(figsize = (20,7))\n",
    "axs3 = fig.add_subplot(111)\n",
    "line1 = axs3.plot(twitter[\"formatted\"], twitter[\"remain_senti\"].values, label = \"Sentiment of pro-remain tweets\", \n",
    "   color = tableau20[0])\n",
    "ax3_twin = axs3.twinx()\n",
    "line2 = ax3_twin.plot(twitter[\"formatted\"], newspaper[\"guard_senti\"], label = \"Sentiment of pro-remain media\", color = tableau20[2])\n",
    "\n",
    "set_legend = line1+line2\n",
    "labs = [line.get_label() for line in set_legend]\n",
    "axs3.legend(set_legend, labs, loc=3, prop={'size': 15})\n",
    "\n",
    "axs3.set_ylabel(\"Sentiment level\",fontsize = 20)\n",
    "axs3.set_xlabel(\"Date\", fontsize = 20)\n",
    "ax3_twin.set_ylabel(\"Sentiment level\",fontsize = 20)\n",
    "plt.xlim('2018-11-13', '2019-06-04')\n",
    "axs3.spines[\"top\"].set_visible(False)\n",
    "ax3_twin.spines[\"top\"].set_visible(False)\n",
    "\n",
    "plt.savefig(\"sentiment_3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig 4(d)\n",
    "fig = plt.figure(figsize = (20,7))\n",
    "axs4 = fig.add_subplot(111)\n",
    "line1 = axs4.plot(twitter[\"formatted\"], twitter[\"leave_senti\"].values, label = \"Sentiment of pro-leave tweets\",\n",
    "   color = tableau20[0])\n",
    "ax4_twin = axs4.twinx()\n",
    "line2 = ax4_twin.plot(twitter[\"formatted\"], newspaper[\"mail_senti\"].values, label = \"Sentiment of pro-leave media\", color = tableau20[2])\n",
    "axs4.set_ylabel(\"Sentiment level\",fontsize = 20)\n",
    "ax4_twin.set_ylabel(\"Sentiment level\",fontsize = 20)\n",
    "\n",
    "set_legend = line1+line2\n",
    "labs = [line.get_label() for line in set_legend]\n",
    "axs4.legend(set_legend, labs, loc=4, prop={'size': 15})\n",
    "axs4.spines[\"top\"].set_visible(False)\n",
    "ax4_twin.spines[\"top\"].set_visible(False)\n",
    "\n",
    "axs4.set_xlabel(\"Date\", fontsize = 20)\n",
    "plt.xlim('2018-11-13', '2019-06-04')\n",
    "plt.savefig(\"sentiment_4.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">Figure 5 & 6: Topic modelling results, daily average proprotion of topics</font size> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment for corpus with all tweets. \n",
    "\n",
    "#leave = pd.read_csv(\"leave_topicmodel.csv\", usecols = [1,2],names = ['date','topic'])\n",
    "leave = pd.read_csv(\"leave_topicmodel_original.csv\", usecols = [1,2])    \n",
    "leave['formatted'] = [datetime.strptime(i, \"%Y-%m-%d %H:%M:%S\") for i in leave[\"date\"]]\n",
    "leave.set_index('formatted', inplace = True)\n",
    "leave['counter'] = [1] * len(leave['topic'])\n",
    "counts = leave['counter'].groupby(pd.Grouper(freq = 'D')).count()\n",
    "\n",
    "leave_calc = pd.DataFrame()\n",
    "leave_calc['counts'] = counts\n",
    "#for i in range(4):\n",
    "for i in range(5):\n",
    "    varname = 'topic_' + str(i)\n",
    "    rolling_name = 'rolling_' + str(i)\n",
    "    count = leave[leave['topic'] == i]['counter'].groupby(pd.Grouper(freq = 'D')).count()\n",
    "    if len(count) != len(counts):\n",
    "        count = pd.concat([pd.Series([0]), count])\n",
    "    percent = [count[k]/counts[k] for k in range(len(count))]\n",
    "    leave_calc[varname] = percent\n",
    "    rolling = leave_calc[varname].rolling(window=3)\n",
    "    rolling_mean = rolling.mean()\n",
    "    leave_calc[rolling_name] = rolling_mean\n",
    "\n",
    "leave_calc = leave_calc.interpolate()\n",
    "#leave_calc.to_csv(\"topic_model_freq_leave_original.csv\")\n",
    "\n",
    "\n",
    "#remain = pd.read_csv(\"remain_topicmodel.csv\", usecols = [1,2], names = ['date','topic'])\n",
    "remain = pd.read_csv(\"remain_topicmodel_original.csv\", usecols = [1,2], names = ['date','topic'])\n",
    "remain['formatted'] = [datetime.strptime(i, \"%Y-%m-%d %H:%M:%S\") for i in remain[\"date\"]]\n",
    "remain.set_index('formatted', inplace = True)\n",
    "remain['counter'] = [1] * len(remain['date'])\n",
    "counts = remain['counter'].groupby(pd.Grouper(freq = 'D')).count()\n",
    "\n",
    "remain_calc = pd.DataFrame()\n",
    "remain_calc['counts'] = counts\n",
    "#for i in range(4):\n",
    "for i in range(3):\n",
    "    varname = 'topic_' + str(i)\n",
    "    rolling_name = 'rolling_' + str(i)\n",
    "    count = remain[remain['topic'] == i]['counter'].groupby(pd.Grouper(freq = 'D')).count()\n",
    "    percent = [count[k]/counts[k] for k in range(len(count))]\n",
    "    remain_calc[varname] = percent\n",
    "    rolling = remain_calc[varname].rolling(window=3)\n",
    "    rolling_mean = rolling.mean()\n",
    "    remain_calc[rolling_name] = rolling_mean\n",
    "remain_calc = remain_calc.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fig 5: all tweets\n",
    "\n",
    "counter = 0\n",
    "plt.figure(figsize = (20,16))\n",
    "ax = plt.subplot(211)    \n",
    "\n",
    "ax.spines[\"top\"].set_visible(False)       \n",
    "ax.spines[\"right\"].set_visible(False)  \n",
    "\n",
    "plt.grid(linestyle = 'dashed')  \n",
    "\n",
    "\n",
    "labels = ['Topic 1: The UK Indepedence Party',\n",
    "          'Topic 2: Brexit Party \\nfor EU election',\n",
    "          'Topic 3: Labours for leave',\n",
    "          'Topic 4: Conservative for leave',\n",
    "          ]\n",
    "\n",
    "for i in leave_calc.columns.values:\n",
    "    if 'topic' in i:\n",
    "        ax.plot(leave_calc.index.values, leave_calc[i].values, color = tableau20[counter],\n",
    "                 label = labels[counter])\n",
    "        counter += 1\n",
    "\n",
    "plt.xlim('2018-11-17', '2019-06-10')\n",
    "plt.legend(prop={'size': 13})\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "ax2 = plt.subplot(212)\n",
    "counter = 0\n",
    "ax2.spines[\"top\"].set_visible(False)       \n",
    "ax2.spines[\"right\"].set_visible(False)  \n",
    "\n",
    "\n",
    "plt.grid(linestyle = 'dashed')  \n",
    "\n",
    "labels_remain = [\"Topic 1: Revoke article 50\",\n",
    "                 \"Topic 2: The liberals for remain\\n and EU election\",\n",
    "                 \"Topic 3: People's Vote\",\n",
    "                 \"Topic 4: Brexit shambles\"]\n",
    "\n",
    "for i in remain_calc.columns.values:\n",
    "    if 'rolling' in i:\n",
    "        ax2.plot(remain_calc.index.values, remain_calc[i].values, color = tableau20[counter],\n",
    "                 label = labels_remain[counter])\n",
    "        counter += 1\n",
    "\n",
    "plt.legend(prop={'size': 13})\n",
    "plt.xlim('2018-11-17', '2019-06-10')\n",
    "plt.xlabel(\"Date\", fontsize = 20)\n",
    "ax.set_ylabel(\"Prob(leave topics)\", fontsize = 20)\n",
    "ax2.set_ylabel(\"Prob(remain topics)\", fontsize = 20)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "#plt.figtext(0.1, 0.01, '''Source: Self-collected twitter data.\n",
    "#            Figures were three-day rolling average; \n",
    "#            missing data points interpolated.''', ha='left', va='bottom') #Uncomment for .png output. This part was in .tex file.\n",
    "\n",
    "#plt.savefig('topic_model_refined.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 6: original only\n",
    "counter = 0\n",
    "plt.figure(figsize = (20,16))\n",
    "ax = plt.subplot(211)    \n",
    "\n",
    "ax.spines[\"top\"].set_visible(False)       \n",
    "ax.spines[\"right\"].set_visible(False)  \n",
    "\n",
    "plt.grid(linestyle = 'dashed')  \n",
    "\n",
    "\n",
    "labels = ['Topic 1: UKIP & Brexit Party \\n EU election',\n",
    "          'Topic 2: Brexit politicians \\n BBC politics',\n",
    "          'Topic 3: Labour for leave',\n",
    "          'Topic 4: Attacking the remainers',\n",
    "          'Topic 5: Right-wing opinion \\nand memes']\n",
    "\n",
    "\n",
    "for i in leave_calc.columns.values:\n",
    "    if 'topic' in i:\n",
    "        ax.plot(leave_calc.index.values, leave_calc[i].values, color = tableau20[counter],\n",
    "                 label = labels[counter])\n",
    "        counter += 1\n",
    "\n",
    "plt.xlim('2018-11-17', '2019-06-10')\n",
    "plt.legend(prop={'size': 13})\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "ax2 = plt.subplot(212)\n",
    "counter = 0\n",
    "ax2.spines[\"top\"].set_visible(False)       \n",
    "ax2.spines[\"right\"].set_visible(False)  \n",
    "\n",
    "\n",
    "plt.grid(linestyle = 'dashed')  \n",
    "\n",
    "labels_remain = [\"Topic 1: People's Vote\",\n",
    "                 \"Topic 2: Revoke article 50\",\n",
    "                 'Topic 3: Brexit shambles']\n",
    "\n",
    "for i in remain_calc.columns.values:\n",
    "    if 'rolling' in i:\n",
    "        ax2.plot(remain_calc.index.values, remain_calc[i].values, color = tableau20[counter],\n",
    "                 label = labels_remain[counter])\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "plt.legend(prop={'size': 13})\n",
    "plt.xlim('2018-11-17', '2019-06-10')\n",
    "plt.xlabel(\"Date\", fontsize = 20)\n",
    "ax.set_ylabel(\"Prob(leave topics)\", fontsize = 20)\n",
    "ax2.set_ylabel(\"Prob(remain topics)\", fontsize = 20)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "#plt.savefig('topic_model_refined_all.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">Figure 7 & 8: ML metrics </font size> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 7: ML metrics (F1)\n",
    "f1 = {'Baseline':0.789,\n",
    "      'Logistic Regression':0.817,\n",
    "      'VADER':0.789,\n",
    "      'Random Forest': 0.940}\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(range(len(f1)),list(f1.values()),align = 'center', width = 0.5, alpha=0.6)\n",
    "plt.xticks(range(len(f1)), list(f1.keys()))\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_xlabel('Methods')\n",
    "ax.spines[\"top\"].set_visible(False)     ,  \n",
    "ax.spines[\"right\"].set_visible(False)  \n",
    "for i, v in enumerate(list(f1.values())):\n",
    "    ax.text(i - .14 , v+.02,str(v), color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 8: topic model u_mass coherence\n",
    "metrics = pd.read_excel(r\"metrics.xlsx\")\n",
    "metrics[\"topics\"] = [i for i in range(2,14)]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "axs[0].plot(metrics['topics'], metrics['twitter-leave-umass'])\n",
    "axs[0].set_xlabel('Number of topics(leave - all)', fontsize = 14)\n",
    "axs[0].set_ylabel('u_mass coherence', fontsize = 14)\n",
    "axs[1].plot(metrics['topics'], metrics['twitter-remain'])\n",
    "axs[1].set_xlabel(\"Number of topics(remain - all)\", fontsize = 14)\n",
    "axs[0].spines[\"top\"].set_visible(False)      \n",
    "axs[1].spines[\"top\"].set_visible(False)      \n",
    "axs[0].spines[\"right\"].set_visible(False)   \n",
    "axs[1].spines[\"right\"].set_visible(False)  \n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
